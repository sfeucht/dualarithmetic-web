<!doctype html>
<html lang="en">

<head>
    <title>Vector Arithmetic in Concept and Token Subspaces</title>
    <meta charset="utf-8"> 
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="description"
        content="What is in a word embedding? Webpage for Vector Arithmetic in Concept and Token Subspaces (Feucht et al., 2025)" />
    <meta property="og:title" content="Vector Arithmetic in Concept and Token Subspaces" />
    <meta property="og:url" content="https://arithmetic.baulab.info/" />
    <meta property="og:image" content="https://arithmetic.baulab.info/images/favicon/android-chrome-192x192.png" />
    <meta property="og:description" content="
    ">
    <meta property="og:type" content="website" />
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Vector Arithmetic in Concept and Token Subspaces" />
    <meta name="twitter:description"
        content="What is in a word embedding? Webpage for Vector Arithmetic in Concept and Token Subspaces (Feucht et al., 2025)." />
    <meta name="twitter:image" content="https://arithmetic.baulab.info/images/favicon/android-chrome-192x192.png" />
    <link rel="icon" href="favicon.ico?v=2" type="image/x-icon"/>
    <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/imagfes/favicon/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">

    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
    <script src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+Math&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
    <link href="style.css" rel="stylesheet">

    <style>
        .relatedthumb {
            float: left;
            width: 200px;
            margin: 3px 10px 7px 0;
        }

        .relatedblock {
            clear: both;
            display: inline-block;
        }

        .bold-sc {
            font-variant: small-caps;
            font-weight: bold;
        }

        .cite,
        .citegroup {
            margin-bottom: 8px;
        }

        :target {
            background-color: yellow;
        }

        .comparison-box {
            border: 2px solid cornflowerblue;
            border-radius: 5px;
            background-color: #e6f0ff; /* light cornflower blue */
            margin: 20px auto;
            max-width: 800px;
            overflow: hidden;
        }
        .box-title {
            background-color: cornflowerblue;
            color: white;
            padding: 10px 15px;
            font-weight: bold;
            font-size: 1.1em;
        }
        .content {
            padding: 15px;
            font-size: 0.9em;
        }
        .divider {
            border-top: 1px solid cornflowerblue;
            margin: 0;
        }
        .original, .ablated {
            margin-bottom: 15px;
        }
        .text {
            margin-top: 5px;
            line-height: 1.5;
        }
        .bold {
            font-weight: bold;
        }
        .underline {
            text-decoration: underline;
            text-decoration-thickness: 2px;
            text-decoration-color: #3366cc;
        }
        .code-container {
            display: flex;
            flex-direction: column;
        }
        @media (min-width: 768px) {
            .code-container {
                flex-direction: row;
            }
        }
        .code-section {
            flex: 1;
            padding: 15px;
        }
        .divider-vertical {
            width: 1px;
            background-color: cornflowerblue;
        }
        .divider-horizontal {
            height: 1px;
            background-color: cornflowerblue;
        }
        pre {
            background-color: #f5f5f5;
            padding: 10px;
            border-radius: 4px;
            overflow-x: auto;
            margin: 0;
            font-family: monospace;
            white-space: pre-wrap;
        }
    </style>
</head>

<body class="nd-docs">
    <div class="nd-pageheader">
        <div class="container">
            <h1 class="lead">
                <nobr class="widenobr">Vector Arithmetic in Concept and Token Subspaces
                </nobr>
            </h1>
            <address>
                <nobr><a href="https://sfeucht.github.io/" target="_blank">Sheridan Feucht</a>,</nobr>
                <nobr><a href="https://www.byronwallace.com/" target="_blank">Byron C. Wallace</a>,</nobr>
                <nobr><a href="https://baulab.info/" target="_blank">David Bau</a></nobr><br>
                <nobr><a href="https://khoury.northeastern.edu/" target="_blank">Northeastern University</a></nobr>
            </address>
        </div>
    </div><!-- end nd-pageheader -->

    <div class="container">
        <div class="row justify-content-center" style="margin-bottom: 20px">
        </div>
        <div class="row justify-content-center text-center">

            <p>
                <a href="literature/arithmetic.pdf" class="d-inline-block p-3 align-top" target="_blank">
                    <img height="100" width="78" src="images/paper_thumb.png" style="border:1px solid; margin: 0 38px;"
                        alt="ArXiv Paper thumbnail" data-nothumb="">
                    <br>Paper PDF</a>
                <a href="https://github.com/sfeucht/arithmetic" class="d-inline-block p-3 align-top" target="_blank">
                    <img height="100" width="78" src="images/code-thumb.png" style="border:1px solid; margin: 0 38px;"
                        alt="Github code thumbnail" data-nothumb="">
                    <br>GitHub<br>Source Code<br>
                </a>
            </p>

        </div><!--row-->

        <div class="row justify-content-center text-center">
            <div class="card" style="max-width: 1020px;">
                <div class="card-block">
                <h3>What is in a word embedding?</h3>
                <p>
                    <figure class="center_image">
                        <img src="images/figure1.png" class="bigfig">
                    </figure>
                    To represent the word <i>boat</i>, large language models (LLMs) have to model a lot of information. Semantically, a boat is a type of vehicle that can usually hold a person and floats on water. But it may also be helpful for an LLM to encode that "boat" is an English word, that it starts with "b," and that it <a href="https://arxiv.org/pdf/2508.02527">rhymes</a> with "oat", "goat", and "float" (McLaughlin et al., 2025). In this short paper, we show how <a href="https://dualroute.baulab.info/">certain model components</a> specialized for copying semantic and token information can be used to tease out these distinctions in the geometry of Llama-2-7b representations. 
                </p>
                </div><!--card-block-->
                </div><!--card-->
        </div>

        <div class="row">
            <div class="col">
                <h2>Background: Word Vector Arithmetic</h2>
                In their famous <a href="https://arxiv.org/abs/1301.3781">word2vec paper</a>, Mikolov et al. (2013) showed that they could train word embeddings that reflected intuitive parallelogram-like structure. The classic example was that of a consistent "gender vector" in embedding space, where they showed that the difference between the vector for <i>man</i> and the vector for <i>woman</i> was basically equal to the difference between <i>king</i> and <i>queen</i>. In other words, if you added <tt>(man - woman)</tt> to <tt>queen</tt>, you could actually get the vector for <tt>king</tt>. 

                <br><br>

                <figure class="center_image">
                    <img src="images/word2vec.png" class="bigfig">
                </figure>  

                You could also think of this in terms of analogies, like "man is to woman as king is to queen," or "Tim Hortons is to Canada as Dunkin' is to New England." But today's LLMs are decoder models with the goal of predicting the next token, not creating embeddings for use in downstream applications. Can we figure out how to do this kind of analysis on a model like Llama-2-7b?

                <h2>Concept and Token Induction</h2>
                In our previous paper on the "Dual-Route Model of Induction" (Feucht et al., 2025), we isolated two types of induction heads, attention heads in LLMs that are responsible for copying text. We found that token induction heads, originally described by <a href="https://transformer-circuits.pub/2021/framework/index.html">Elhage et al. (2021)</a>, are responsible for verbatim copying, whereas concept induction heads were responsible for copying whole word representations. 

                <figure class="center_image">
                    <img src="images/david_figure1_cropped.jpg" class="bigfig">
                </figure>  

                In that paper, we used the weights of those two types of heads to create <b>concept lens</b> and <b>token lens</b>, matrices that can reveal either semantic or literal token information stored in a given hidden state. (See our <a href="https://dualroute.baulab.info/#lens">original project page</a> for a quick overview of this approach, or the paper for specific details).

                <h2>Our Approach</h2>
                In this work, we investigate whether we can use Llama-2-7b's internal hidden states as word embeddings that operate like Mikolov et al. (2013)'s vectors. Let's say we want to see if Llama-2-7b's hidden states encode country-capital city relationships. For every country and capital in their dataset, we pass that word through Llama-2-7b (with the prefix "She travelled to" to ensure that it understands each word as a location). We can then take the activations for that word at the last token position as an embedding for that word, and perform word2vec arithmetic with all of these separate hidden states.

                <br><br>

                <figure class="center_image">
                    <img src="images/rawmethod.png" class="bigfig">
                </figure>  

                The best nearest-neighbor accuracy you can get by doing this with raw hidden states is around 50%. However, we find that if you use <b>concept lens</b> to transform these hidden states first, you can get accuracy comparable with the model's performance when prompted to do the same thing in-context! (See Figure 1 of <a href="literature/dual_arithmetic.pdf">the paper</a> for full results.)

                <figure class="center_image">
                    <img src="images/conceptmethod.png" class="bigfig">
                </figure>  

                Finally, if we do the same thing for <b>token lens</b>, then analogies like <tt>code - coding = dance - dancing</tt> become much clearer (with around 80% accuracy) compared to using raw hidden states (around 30% accuracy). Again, see Figure 1 of <a href="literature/dual_arithmetic.pdf">the paper</a> for full results.

                <br><br>
                 <figure class="center_image">
                    <img src="images/tokenmethod.png" class="bigfig">
                </figure>  

                These results suggest that the concept and token heads found in previous work don't just blindly copy representations of tokens, or words. In the case of concept heads, it seems that they transport rich representations of the semantics of a word, whereas for token heads, they transport some kind of information that has to do with how words are literally written. 

                <h2 id="related-work">Related Work</h2>

                <p class="citation"><a href="https://arxiv.org/abs/2305.16130"><img src="images/jack_word2vec.png" alt="Figure 1 from Merullo et al. (2023)">Language Models Implement Simple Word2Vec-style Vector Arithmetic. Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. 2023.</a><br>
                <b>Notes:</b> Merullo et al. (2023) find a very similar phenomenon in GPT2-Medium: they show that feed-forward sublayers in later layers of the model will actually output <tt>get_capital(x)</tt> function vectors (similar to <a href="https://functions.baulab.info/">Todd et al. (2024)</a>) that, when added to <tt>Poland</tt>, cause the model to output <tt>Warsaw</tt>. These may be the <i>very same</i> vectors we obtain when calculating <tt>Athens - Greece</tt> in concept space, because they occur at mid-late layers, after concept induction has taken place. In other words, we speculate that the feed-forward components of later model layers have learned to work within the output subspace of concept induction heads, taking advantage of this semantic rich structure to perform word2vec-style operations.
                </p>          

                <p class="citation"><a href="https://arxiv.org/abs/2508.02527"><img src="images/phonemes.png" alt="Figure 1 from McLaughlin et al. (2025)"> Oliver McLaughlin, Arjun Khurana, and Jack Merullo. I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic Representations in LLaMA 3.2. 2025.</a><br>
                    <b>Notes:</b> Where we focus on semantic and token-level representations of words in this work, McLaughlin et al. find evidence of models also representing <i>phonemic</i> information about words, despite having no explicit access to how words actually sound. This is another example of LLMs encoding implicit information about words that has little to do with the semantics of those words.
                    </p>
     
                <p class="citation"><a href="https://dualroute.baulab.info/"><img src="images/dualroute.png" alt="Figure 1 from Feucht et al. (2025)">Sheridan Feucht, Eric Todd, Byron Wallace, and David Bau. The Dual-Route Model of Induction. 2025.</a><br>
                    <b>Notes:</b> Our own previous work: we use the same concept/token lens approach described in Section 5.
                    </p>  

                <h2>How to cite</h2>

                <p>This work was accepted at the NeurIPS Mech Interp Workshop (2025). It can be cited as follows:
                </p>

                <div class="card">
                    <h3 class="card-header">bibliography</h3>
                    <div class="card-block"> 
                        <p style="text-indent: -3em; margin-left: 3em;" class="card-text clickselect">
                            Sheridan Feucht, Byron Wallace, and David Bau. "<i>Vector Arithmetic in Concept and Token Subspaces.</i>" Second Mechanistic Interpretability Workshop at NeurIPS (2025).</nobr> 
                        </p>
                    </div>
                    <h3 class="card-header">bibtex</h3>
                    <div class="card-block">
                        <pre class="card-text clickselect"
                        >@inproceedings{feucht2025arithmetic,
  title={Vector Arithmetic in Concept and Token Subspaces},
  author={Sheridan Feucht and Byron Wallace and David Bau},
  booktitle={Second Mechanistic Interpretability Workshop at NeurIPS},
  year={2025},
  url={https://arithmetic.baulab.info}
}</pre>
                    </div>
                </div>

            </div> <!--col -->    
        </div> <!--row -->
    </div> <!-- container -->

    

    <footer class="nd-pagefooter">
        <div class="row">
            <div class="col-6 col-md text-center">
                <a href="https://baulab.info/">About the Bau Lab</a>
            </div>
        </div>
    </footer>

</body>

</html>

